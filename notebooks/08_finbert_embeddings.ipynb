{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load day-level docs (so you only embed one document per day)\n",
    "df = pd.read_parquet(\"../data/processed/model_table_clean.parquet\").copy()\n",
    "df[\"trading_date\"] = pd.to_datetime(df[\"trading_date\"])\n",
    "df = df.sort_values(\"trading_date\")\n",
    "\n",
    "day_df = (\n",
    "    df.groupby(\"trading_date\")\n",
    "      .agg(\n",
    "          doc=(\"clean_headline\", lambda x: \" \".join(x.tolist())),\n",
    "          return_t_plus_1=(\"return_t_plus_1\", \"first\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "day_df[\"label\"] = (day_df[\"return_t_plus_1\"] > 0).astype(int)\n",
    "\n",
    "# Drop early days without labels if needed\n",
    "day_df = day_df.dropna().copy()\n",
    "\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Freeze model (no gradients needed)\n",
    "base_model.eval()\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Days:\", len(day_df))\n",
    "\n",
    "def embed_texts(texts, max_length=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Compute FinBERT embeddings for a list of texts using mean pooling of last hidden state.\n",
    "\n",
    "    We do mean pooling because it is simple, stable, and works well as a generic embedding.\n",
    "    \"\"\"\n",
    "    all_vecs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i+batch_size]\n",
    "\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            out = base_model(**enc)\n",
    "            # out.last_hidden_state: (batch, seq, hidden)\n",
    "            hidden = out.last_hidden_state\n",
    "\n",
    "            # Attention mask to avoid pooling over padding tokens\n",
    "            mask = enc[\"attention_mask\"].unsqueeze(-1)  # (batch, seq, 1)\n",
    "            masked_hidden = hidden * mask\n",
    "\n",
    "            # Mean pooling\n",
    "            sum_hidden = masked_hidden.sum(dim=1)\n",
    "            denom = mask.sum(dim=1).clamp(min=1e-9)\n",
    "            mean_pooled = sum_hidden / denom\n",
    "\n",
    "            all_vecs.append(mean_pooled.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_vecs)\n",
    "\n",
    "embeddings = embed_texts(day_df[\"doc\"].tolist(), max_length=128, batch_size=32)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "os.makedirs(\"../data/features\", exist_ok=True)\n",
    "\n",
    "emb_df = pd.DataFrame(embeddings)\n",
    "emb_df.insert(0, \"trading_date\", day_df[\"trading_date\"].values)\n",
    "emb_df.insert(1, \"label\", day_df[\"label\"].values)\n",
    "emb_df.insert(2, \"return_t_plus_1\", day_df[\"return_t_plus_1\"].values)\n",
    "\n",
    "out_path = \"../data/features/finbert_day_embeddings.parquet\"\n",
    "emb_df.to_parquet(out_path, index=False)\n",
    "\n",
    "print(\"Saved:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
